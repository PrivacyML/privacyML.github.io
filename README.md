privacy.github.io | ML Privacy &amp; Privacy Eval | A Tutorial at NeurIPS 2024 | Vancouver, British Columbia, Canada
## Meaningful Privacy-Preserving Machine Learning and How To Evaluate AI Privacy
In the world of large model development, model details and training data are increasingly closed down, pushing privacy to the forefront of machine learning – how do we protect privacy of the data used to train the model, permitting more widespread data sharing collaborations? How will individuals trust these technologies with their data? How do we verify that the integration of individual’s data is both useful to the rest of the participating federation, and, more importantly - safe for the data owner? How do the regulations integrate into this complex infrastructure?

These open questions require a multitude of considerations between the incentives of model development,the  data owning parties, and the overseeing agencies. Many cryptographic solutions target these incentives problems, but  are they covering all essential components of trustworthy data sharing? Are they practical, or likely to be practical soon?

In this tutorial, we attempt to answer questions regarding specific capabilities of privacy technologies in three parts: 1. overarching incentive issues with respect to data and evaluations, 2. Where cryptographic and optimisation solutions can help; for evaluations, we delve deep into secure computation and machine unlearning. 3. Cultural, societal, and research agendas relating to practically implementing these technologies.

We hope that, by identifying the boundaries of the use of privacy technologies, and providing a technical and structured framework for reasoning over these issues, we could empower the general audience to integrate these principles (and practical solutions) into their existing research. Those already interested in applying the technology can gain a deeper, hands-on understanding of implementation useful for modeling and developing incentive-compatible solutions for their own work.

# Why Tutorial
We observe that the ML community cannot sufficiently access privacy tech. People interested in safe ML topics such as model alignment often do not have a clear understanding of cryptography-based techniques when applied to ML.

On the other hand, cryptographers want to apply their knowledge, especially with practical security management, to machine learning systems. Yet they face the problem of a completely different mindset from machine learning researchers who currently drive the development of these systems.

These communities do not engage with each other, partly because they do not have a shared common ground. This tutorial can bridge the gap between cryptography and effective decentralized ML training and evaluation.

Through our work on evaluations of privacy technology for machine learning, we found that disparate communities in ML training, large model R&D, safety and policy urgently want to have an overview on the state of privacy and evaluations that suit their purpose, especially as generative AI technologies become important. However, there is no single technology that fits all use cases, and without in-depth knowledge of both machine learning and cryptography, it can be very difficult to initiate research in this impactful and fast-moving field. We hope that clarifying the technologies for this purpose will help towards effective scientific coordination.

# Speakers
Mimee Xu (​mimee@nyu.edu​)​: ​Mimee Xu is a 6th year PhD student at NYU’s Courant Institute, working under Prof. Leon Bottou on the intersection of Privacy and Security of ML. During her PhD, she interned at Facebook AI Research and Bytedance Machine Learning. Previously, she worked as a Research Engineer at Baidu SVAIL, UnifyID, and before that, she worked on security and stability on the Google Chrome team. She organized the ML for Systems workshop for 5 years.

Fazl Barez (fazl@robots.ox.ac.uk) is a Research Fellow at Torr Vision Group (TVG), University of Oxford, where he works on topics related to AI safety. Additionally, Fazl also holds affiliations with Kruger AI Safety Lab (KASL), the Centre for the Study of Existential Risk at University of Cambridge and Future of Life Institute.  Previously, Fazl  worked as Technology and Security Policy Fellow at RAND, on Interpretability at Amazon and The DataLab, safe recommender systems at Huawei, and on building a finance tool for budget management for economic scenario forecasting at Natwest Group. Fazl holds a PhD in Artificial Intelligence Safety and has previously organized a workshop at ICML (Mechnatic Interpretability) and a workshop at EACL (Inverse Scaling)

Dmitrii Usynin is a Research Scientist at Oblivious and a final year PhD student in a Joint Academy of Doctoral Studies (JADS) at Imperial College London and TU Munich. His research interests lie on the intersection of collaborative machine learning (CML) and trustworthy artificial intelligence. In particular, he is interested in topics such as privacy-preserving machine learning (PPML), attacks on CML, federated learning and memorisation in ML. Previously he was a ML Researcher at Microsoft Research and Brave Research. At MSR he worked on memorisation and factuality for radiology-facing LLMs. At Brave Research he worked on principled selection of clients and data in resource-constrained federated learning. In addition he was also a Privacy Researcher at OpenMined, working on federated learning and differential privacy in healthcare.
